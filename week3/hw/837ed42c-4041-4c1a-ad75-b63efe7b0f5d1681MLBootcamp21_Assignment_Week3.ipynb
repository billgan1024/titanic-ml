{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "MLBootcamp21_Assignment_Week3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.9.5 64-bit"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "interpreter": {
      "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "ML Bootcamp 2021 - Week 3: Assignment\n",
        "\n",
        "- In this week you will learn more about the Pandas and specifically use it to create features\n",
        "- Finally these features will be used to train a Logistic Regression model and you will submit your solution to the Kaggle ****Titanic Machine Learning from Disaster**** competition\n",
        "- The comments will have most of the instructions\n",
        "- Attempt the questions using only python/pandas as much as possible as it is a very in-demand skill right now and learning it will be beneficial\n",
        "\n",
        "## USE TRAIN DATASET FROM THE TITANIC DATASET PROVIDED ONLY UNLESS SPECIFIED"
      ],
      "metadata": {
        "id": "vEslPzD4HNPG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 191,
      "source": [
        "# Import the necessary Libraries here\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import re\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "from sklearn.metrics import classification_report\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "df_train = pd.read_csv(\"train.csv\")\r\n",
        "df_test = pd.read_csv(\"test.csv\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "YyU4EAcwHTAt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q1. What is the average ticket price per ticket class(Pclass) for each individual port of embarkation? In other words, what is the price of each ticket class for each category of \"Embarked\" column?\n",
        "\n",
        "## Your answer should be in the following format as a Python Dictionary\n",
        "\n",
        "{<br>\n",
        "  (Pclass,Embarked Category) : Avg Fare<br>\n",
        "}\n",
        "\n",
        "Eg:\n",
        "\n",
        "{<br>\n",
        " (1, 'C'): Avg Fare,<br>\n",
        " (1, 'Q'): Avg Fare,<br>\n",
        " (1, 'S'): Avg Fare,<br>\n",
        " .<br>\n",
        " .<br>\n",
        " .<br>\n",
        " so on..<br>\n",
        "}\n",
        "\n",
        "\n",
        "## You can verify your answer as follows: \n",
        " - For the \"Embarked\" category 'Q' and 'Pclass' of 1 the average ticket price is \\$ 90\n",
        " - For the \"Embarked\" category 'S' and 'Pclass' of 3 the average ticket price is \\$ 14.64"
      ],
      "metadata": {
        "id": "AAuyyEi1HmR_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 192,
      "source": [
        "#finds mean of (C, P1)'s fares, (C, P2)'s fares, and so on\r\n",
        "df_train.groupby(by=[\"Pclass\", \"Embarked\"])[\"Fare\"].mean().to_dict()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{(1, 'C'): 104.71852941176469,\n",
              " (1, 'Q'): 90.0,\n",
              " (1, 'S'): 70.36486220472443,\n",
              " (2, 'C'): 25.358335294117644,\n",
              " (2, 'Q'): 12.35,\n",
              " (2, 'S'): 20.327439024390245,\n",
              " (3, 'C'): 11.214083333333337,\n",
              " (3, 'Q'): 11.183393055555557,\n",
              " (3, 'S'): 14.64408300283288}"
            ]
          },
          "metadata": {},
          "execution_count": 192
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q2. Create the \"Title\" feature in your dataset as an additional column and call it \"Title\". This \"Title\" will be extracted from the name and will be the Title given to each name which can be \"Mr.\", \"Mrs.\", \"Dr.\" and many more. Refer to the last question of Week 1 Assignment for reference\n",
        "\n",
        "- Only create the extra column called \"Title\" in your existing dataframe, no need to report any answer as you will be using all these features in the next few questions"
      ],
      "metadata": {
        "id": "oph5WCmrKKFj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 193,
      "source": [
        "# Try to use regular expressions and use the pandas .apply() lambda function to solve \r\n",
        "# this as these are very powerful functions that will benefit you if learned now\r\n",
        "df_train[\"Title\"] = df_train[\"Name\"].apply(lambda x: re.findall(\"[A-Za-z]+\\.\", x)[0])\r\n",
        "df_test[\"Title\"] = df_test[\"Name\"].apply(lambda x: re.findall(\"[A-Za-z]+\\.\", x)[0])"
      ],
      "outputs": [],
      "metadata": {
        "id": "P8WMMM6uIh7y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q3. Use the dataframe from above, which should include the \"Title\" column now, if you have successfully completed Q2. Encode this \"Title\" column using any encoding technique of your choice. Explore the various encoding techniques by reading this article:\n",
        "\n",
        "https://www.kdnuggets.com/2021/05/deal-with-categorical-data-machine-learning.html\n",
        "\n",
        "\n",
        "- As a start you can use any of the following techniques but there are many more\n",
        "  - pandas get_dummies()\n",
        "  - One hot encoding\n",
        "  - Label encoding\n",
        "- Explore each of the above techniques as you will learn a lot just by reading them\n"
      ],
      "metadata": {
        "id": "m9Eufd9xKpI9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "source": [
        "for s in np.union1d(df_train[\"Title\"].unique(), df_test[\"Title\"].unique()):\r\n",
        "    df_train[s] = df_train[\"Title\"].apply(lambda x : 1 if x == s else 0)\r\n",
        "    df_test[s] = df_test[\"Title\"].apply(lambda x : 1 if x == s else 0)\r\n"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'np' is not defined",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-1-48a593b6e31f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munion1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Title\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Title\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mdf_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Title\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mdf_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Title\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q4. This is an open ended Optional question. Explore the data as we have done for Week 3 class and create a new feature of your own. Add it as a new column to the dataframe. \n",
        "\n",
        "- This feature might help you get a good score for your Kaggle submission"
      ],
      "metadata": {
        "id": "FJ2f7IRsLsGe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 195,
      "source": [
        "# Follow the same process as the previous class notebooks on creating new columns. \r\n",
        "# This feature can be as simple as combining two columns by multiplying/dividing them with each other\r\n",
        "# Feel free to have a discussion in the group as this is a tricky Feature Engineering problem and might not be straightforward, hence optional."
      ],
      "outputs": [],
      "metadata": {
        "id": "MWATjfkyLPEt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q5. Create a LogisticRegression model as done in class. Use the dataset you have developed after answering all the previous questions(make sure all the data is only numerical). Train your model on the \"train.csv\" file, and then make predictions on your \"test.csv\" file. Format your output EXACTLY as shown in class at the very end and EXACTLY like the \"gender_submission.csv\" file provided as part of the challenge\n",
        "\n",
        "- Once you have created your own submission file, submit it on Kaggle for evaluation\n",
        "- Report the score as obtained from the kaggle website here and store it in a variable called ```result```"
      ],
      "metadata": {
        "id": "UAT438zOMD3-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 196,
      "source": [
        "# drop the data\n",
        "df_train.fillna(method=\"ffill\", inplace=True)\n",
        "df_test.fillna(method=\"ffill\", inplace=True) \n",
        "drop_features = [\"PassengerId\", \"Sex\",'Ticket','Name','Cabin',\"Embarked\", \"Title\"]\n",
        "df_train.drop(drop_features, inplace=True, axis=1)\n",
        "\n",
        "drop_features = [\"Sex\",'Ticket','Name','Cabin',\"Embarked\", \"Title\"]\n",
        "df_test.drop(drop_features, inplace=True, axis=1)\n",
        "\n",
        "\n",
        "model = LogisticRegression()\n",
        "x_train, x_test, y_train, y_test = train_test_split(df_train.loc[:,'Pclass':],df_train[\"Survived\"],test_size=0.2)\n",
        "model.fit(x_train,y_train)\n",
        "\n",
        "predictions = model.predict(df_test.loc[:,\"Pclass\":])\n",
        "\n",
        "#note: slicing using a string returns a series, and slicing using an array returns a dataframe\n",
        "df_submission = df_test[['PassengerId']].copy()\n",
        "df_submission['Survived'] = predictions\n",
        "df_submission.to_csv(\"submission.csv\", index=False)\n",
        "\n",
        "result = 0.77033\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ],
      "metadata": {
        "id": "QQEzAarlMyzK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {}
    }
  ]
}